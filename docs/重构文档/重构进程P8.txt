## Phase 8: éƒ¨ç½²ä¸è¿ç»´ï¼ˆ3å¤©ï¼‰

### ğŸ“… æ—¶é—´å®‰æ’
- **Day 1**: Milestone 8.1 CI/CDæµæ°´çº¿æ­å»º
- **Day 2**: Milestone 8.2 å®¹å™¨åŒ–ä¸ç¼–æ’
- **Day 3**: Milestone 8.3 è‡ªåŠ¨åŒ–è¿ç»´

---

## Milestone 8.1: CI/CDæµæ°´çº¿æ­å»ºï¼ˆ1å¤©ï¼‰

### Day 1: æµæ°´çº¿å®ç°ï¼ˆ8å°æ—¶ï¼‰

#### 1. GitLab CI/CDé…ç½®
```yaml
# .gitlab-ci.yml
stages:
  - build
  - test
  - quality
  - deploy
  - rollback

variables:
  DOCKER_REGISTRY: registry.gitlab.com
  IMAGE_NAME: $CI_PROJECT_PATH
  NODE_VERSION: "18"
  CACHE_VERSION: "v1"

# ç¼“å­˜é…ç½®
.cache_template: &cache_config
  cache:
    key: "$CACHE_VERSION-$CI_COMMIT_REF_SLUG"
    paths:
      - node_modules/
      - .npm/
      - dist/

# æ„å»ºé˜¶æ®µ
build:
  stage: build
  image: node:${NODE_VERSION}
  <<: *cache_config
  before_script:
    - npm config set registry https://registry.npmmirror.com
    - npm ci --cache .npm --prefer-offline
  script:
    - echo "Building version $CI_COMMIT_SHORT_SHA"
    - npm run build:${CI_COMMIT_REF_NAME}
    - echo "$CI_COMMIT_SHORT_SHA" > dist/version.txt
  artifacts:
    paths:
      - dist/
    expire_in: 1 week
  only:
    - develop
    - staging
    - main
    - /^release\/.*$/

# å•å…ƒæµ‹è¯•
test:unit:
  stage: test
  image: node:${NODE_VERSION}
  <<: *cache_config
  coverage: '/Lines\s*:\s*(\d+\.\d+)%/'
  script:
    - npm run test:unit -- --coverage
    - npm run test:integration
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage/cobertura-coverage.xml
      junit: junit.xml
    paths:
      - coverage/
    expire_in: 30 days
  only:
    - merge_requests
    - develop
    - main

# ä»£ç è´¨é‡æ£€æŸ¥
quality:lint:
  stage: quality
  image: node:${NODE_VERSION}
  <<: *cache_config
  script:
    - npm run lint -- --format json --output-file eslint-report.json
    - npm run type-check
  artifacts:
    reports:
      codequality: eslint-report.json
    expire_in: 1 week
  allow_failure: true

# å®‰å…¨æ‰«æ
quality:security:
  stage: quality
  image: node:${NODE_VERSION}
  script:
    - npm audit --json > npm-audit.json || true
    - npx snyk test --json > snyk-report.json || true
  artifacts:
    paths:
      - npm-audit.json
      - snyk-report.json
    expire_in: 30 days
  allow_failure: true

# SonarQubeæ‰«æ
quality:sonar:
  stage: quality
  image: sonarsource/sonar-scanner-cli:latest
  script:
    - sonar-scanner
      -Dsonar.projectKey=$CI_PROJECT_NAME
      -Dsonar.sources=src
      -Dsonar.host.url=$SONAR_HOST_URL
      -Dsonar.login=$SONAR_TOKEN
      -Dsonar.javascript.lcov.reportPaths=coverage/lcov.info
  dependencies:
    - test:unit
  only:
    - develop
    - main

# Dockeré•œåƒæ„å»º
.docker_build_template: &docker_build
  stage: build
  image: docker:latest
  services:
    - docker:dind
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    - docker build 
      --cache-from $DOCKER_REGISTRY/$IMAGE_NAME:$CI_COMMIT_REF_NAME
      --tag $DOCKER_REGISTRY/$IMAGE_NAME:$CI_COMMIT_SHORT_SHA
      --tag $DOCKER_REGISTRY/$IMAGE_NAME:$CI_COMMIT_REF_NAME
      --build-arg BUILD_VERSION=$CI_COMMIT_SHORT_SHA
      --build-arg BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ')
      .
    - docker push $DOCKER_REGISTRY/$IMAGE_NAME:$CI_COMMIT_SHORT_SHA
    - docker push $DOCKER_REGISTRY/$IMAGE_NAME:$CI_COMMIT_REF_NAME

docker:build:develop:
  <<: *docker_build
  only:
    - develop

docker:build:staging:
  <<: *docker_build
  only:
    - staging

docker:build:production:
  <<: *docker_build
  only:
    - main
    - tags

# éƒ¨ç½²åˆ°å¼€å‘ç¯å¢ƒ
deploy:dev:
  stage: deploy
  image: alpine/helm:latest
  environment:
    name: development
    url: https://dev.example.com
  before_script:
    - apk add --no-cache curl
  script:
    - |
      helm upgrade --install $CI_PROJECT_NAME ./helm
        --namespace=dev
        --set image.tag=$CI_COMMIT_SHORT_SHA
        --set image.repository=$DOCKER_REGISTRY/$IMAGE_NAME
        --set ingress.host=dev.example.com
        --values ./helm/values.dev.yaml
        --wait
        --timeout 5m
  only:
    - develop

# éƒ¨ç½²åˆ°é¢„å‘å¸ƒç¯å¢ƒ
deploy:staging:
  stage: deploy
  environment:
    name: staging
    url: https://staging.example.com
    on_stop: deploy:staging:stop
  script:
    - ./scripts/deploy.sh staging $CI_COMMIT_SHORT_SHA
  when: manual
  only:
    - staging

# éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
deploy:production:
  stage: deploy
  environment:
    name: production
    url: https://example.com
  script:
    - ./scripts/deploy.sh production $CI_COMMIT_SHORT_SHA
  when: manual
  only:
    - main
    - tags
  before_script:
    - echo "Production deployment approval required"

# å›æ»šæœºåˆ¶
rollback:production:
  stage: rollback
  environment:
    name: production
  script:
    - ./scripts/rollback.sh production
  when: manual
  only:
    - main
```

#### 2. éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# scripts/deploy.sh

set -e

ENVIRONMENT=$1
VERSION=$2

# é…ç½®
source ./config/${ENVIRONMENT}.env

echo "Deploying version ${VERSION} to ${ENVIRONMENT}"

# å¥åº·æ£€æŸ¥å‡½æ•°
health_check() {
    local url=$1
    local max_attempts=30
    local attempt=0
    
    while [ $attempt -lt $max_attempts ]; do
        if curl -f -s "${url}/health" > /dev/null; then
            echo "Health check passed"
            return 0
        fi
        
        attempt=$((attempt + 1))
        echo "Health check attempt ${attempt}/${max_attempts}"
        sleep 10
    done
    
    echo "Health check failed"
    return 1
}

# è“ç»¿éƒ¨ç½²
blue_green_deploy() {
    # éƒ¨ç½²åˆ°ç»¿ç¯å¢ƒ
    echo "Deploying to green environment..."
    kubectl set image deployment/${APP_NAME}-green \
        ${APP_NAME}=${DOCKER_REGISTRY}/${IMAGE_NAME}:${VERSION} \
        -n ${NAMESPACE}
    
    # ç­‰å¾…ç»¿ç¯å¢ƒå°±ç»ª
    kubectl rollout status deployment/${APP_NAME}-green -n ${NAMESPACE}
    
    # å¥åº·æ£€æŸ¥
    if ! health_check "http://${GREEN_URL}"; then
        echo "Green environment health check failed"
        exit 1
    fi
    
    # åˆ‡æ¢æµé‡
    echo "Switching traffic to green..."
    kubectl patch service ${APP_NAME} \
        -p '{"spec":{"selector":{"version":"green"}}}' \
        -n ${NAMESPACE}
    
    # éªŒè¯åˆ‡æ¢
    sleep 5
    if ! health_check "http://${PROD_URL}"; then
        echo "Production health check failed after switch"
        # å›æ»š
        kubectl patch service ${APP_NAME} \
            -p '{"spec":{"selector":{"version":"blue"}}}' \
            -n ${NAMESPACE}
        exit 1
    fi
    
    # æ›´æ–°è“ç¯å¢ƒï¼ˆç”¨äºä¸‹æ¬¡éƒ¨ç½²ï¼‰
    echo "Updating blue environment for next deployment..."
    kubectl set image deployment/${APP_NAME}-blue \
        ${APP_NAME}=${DOCKER_REGISTRY}/${IMAGE_NAME}:${VERSION} \
        -n ${NAMESPACE}
}

# é‡‘ä¸é›€éƒ¨ç½²
canary_deploy() {
    local canary_weight=10
    
    echo "Starting canary deployment..."
    
    # éƒ¨ç½²é‡‘ä¸é›€ç‰ˆæœ¬
    kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ${APP_NAME}-canary
  namespace: ${NAMESPACE}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ${APP_NAME}
      version: canary
  template:
    metadata:
      labels:
        app: ${APP_NAME}
        version: canary
    spec:
      containers:
      - name: ${APP_NAME}
        image: ${DOCKER_REGISTRY}/${IMAGE_NAME}:${VERSION}
        ports:
        - containerPort: 3000
EOF
    
    # é…ç½®æµé‡åˆ†é…
    kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: ${APP_NAME}
  namespace: ${NAMESPACE}
spec:
  hosts:
  - ${APP_NAME}
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: ${APP_NAME}
        subset: canary
  - route:
    - destination:
        host: ${APP_NAME}
        subset: stable
      weight: $((100 - canary_weight))
    - destination:
        host: ${APP_NAME}
        subset: canary
      weight: ${canary_weight}
EOF
    
    echo "Canary deployment started with ${canary_weight}% traffic"
}

# ä¸»æµç¨‹
case ${ENVIRONMENT} in
    development)
        kubectl set image deployment/${APP_NAME} \
            ${APP_NAME}=${DOCKER_REGISTRY}/${IMAGE_NAME}:${VERSION} \
            -n dev
        kubectl rollout status deployment/${APP_NAME} -n dev
        ;;
    
    staging)
        blue_green_deploy
        ;;
    
    production)
        if [ "${DEPLOY_STRATEGY}" = "canary" ]; then
            canary_deploy
        else
            blue_green_deploy
        fi
        
        # å‘é€éƒ¨ç½²é€šçŸ¥
        curl -X POST ${SLACK_WEBHOOK_URL} \
            -H 'Content-Type: application/json' \
            -d "{
                \"text\": \"ğŸš€ Deployed version ${VERSION} to ${ENVIRONMENT}\",
                \"attachments\": [{
                    \"color\": \"good\",
                    \"fields\": [
                        {\"title\": \"Environment\", \"value\": \"${ENVIRONMENT}\"},
                        {\"title\": \"Version\", \"value\": \"${VERSION}\"},
                        {\"title\": \"Deployed by\", \"value\": \"${GITLAB_USER_NAME}\"}
                    ]
                }]
            }"
        ;;
    
    *)
        echo "Unknown environment: ${ENVIRONMENT}"
        exit 1
        ;;
esac

echo "Deployment completed successfully"
```

---

## Milestone 8.2: å®¹å™¨åŒ–ä¸ç¼–æ’ï¼ˆ1å¤©ï¼‰

### Day 2: Dockerå’ŒKubernetesé…ç½®ï¼ˆ8å°æ—¶ï¼‰

#### 1. Dockerfileä¼˜åŒ–
```dockerfile
# Dockerfile
# å¤šé˜¶æ®µæ„å»º
FROM node:18-alpine AS builder

# å®‰è£…æ„å»ºä¾èµ–
RUN apk add --no-cache python3 make g++

WORKDIR /app

# ç¼“å­˜ä¾èµ–å±‚
COPY package*.json ./
RUN npm ci --only=production && \
    npm cache clean --force

# æ„å»ºåº”ç”¨
COPY . .
RUN npm run build

# ç”Ÿäº§é•œåƒ
FROM node:18-alpine AS production

# å®‰å…¨åŠ å›º
RUN apk add --no-cache tini && \
    addgroup -g 1001 -S nodejs && \
    adduser -S nodejs -u 1001

WORKDIR /app

# ä»æ„å»ºé˜¶æ®µå¤åˆ¶
COPY --from=builder --chown=nodejs:nodejs /app/dist ./dist
COPY --from=builder --chown=nodejs:nodejs /app/node_modules ./node_modules
COPY --chown=nodejs:nodejs package*.json ./

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD node healthcheck.js || exit 1

# è¿è¡Œæ—¶é…ç½®
ENV NODE_ENV=production
ENV PORT=3000

EXPOSE 3000

USER nodejs

# ä½¿ç”¨tiniä½œä¸ºinitè¿›ç¨‹
ENTRYPOINT ["/sbin/tini", "--"]
CMD ["node", "dist/server.js"]
```

#### 2. Kubernetesé…ç½®
```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deployment
  namespace: production
  labels:
    app: myapp
    version: v1
spec:
  replicas: 3
  revisionHistoryLimit: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "3000"
        prometheus.io/path: "/metrics"
    spec:
      # åäº²å’Œæ€§ï¼Œåˆ†æ•£åˆ°ä¸åŒèŠ‚ç‚¹
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - myapp
              topologyKey: kubernetes.io/hostname
      
      # å®‰å…¨ä¸Šä¸‹æ–‡
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        fsGroup: 1001
      
      containers:
      - name: app
        image: registry.example.com/myapp:latest
        imagePullPolicy: Always
        
        # èµ„æºé™åˆ¶
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        
        # ç¯å¢ƒå˜é‡
        env:
        - name: NODE_ENV
          value: "production"
        - name: DB_HOST
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: host
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password
        
        # é…ç½®æŒ‚è½½
        volumeMounts:
        - name: config
          mountPath: /app/config
          readOnly: true
        - name: cache
          mountPath: /app/cache
        
        # å¥åº·æ£€æŸ¥
        livenessProbe:
          httpGet:
            path: /health/live
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        
        # ç”Ÿå‘½å‘¨æœŸé’©å­
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]
      
      volumes:
      - name: config
        configMap:
          name: app-config
      - name: cache
        emptyDir:
          sizeLimit: 1Gi
      
      # é•œåƒæ‹‰å–å‡­è¯
      imagePullSecrets:
      - name: registry-secret

---
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: app-service
  namespace: production
  labels:
    app: myapp
spec:
  type: ClusterIP
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 3600
  ports:
  - port: 80
    targetPort: 3000
    protocol: TCP
    name: http
  selector:
    app: myapp

---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-deployment
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
```

---

## Milestone 8.3: è‡ªåŠ¨åŒ–è¿ç»´ï¼ˆ1å¤©ï¼‰

### Day 3: è¿ç»´è„šæœ¬å’Œå·¥å…·ï¼ˆ8å°æ—¶ï¼‰

#### 1. è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬
```python
#!/usr/bin/env python3
# scripts/ops_automation.py

import os
import sys
import json
import time
import logging
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import requests
import yaml
from kubernetes import client, config
from prometheus_client.parser import text_string_to_metric_families

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OpsAutomation:
    def __init__(self, config_file: str):
        with open(config_file, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # åˆå§‹åŒ–Kuberneteså®¢æˆ·ç«¯
        if os.getenv('KUBERNETES_SERVICE_HOST'):
            config.load_incluster_config()
        else:
            config.load_kube_config()
        
        self.k8s_apps = client.AppsV1Api()
        self.k8s_core = client.CoreV1Api()
    
    def health_check(self) -> Dict[str, bool]:
        """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
        results = {}
        
        # æ£€æŸ¥éƒ¨ç½²çŠ¶æ€
        deployments = self.k8s_apps.list_namespaced_deployment(
            namespace=self.config['namespace']
        )
        
        for deployment in deployments.items:
            name = deployment.metadata.name
            ready = deployment.status.ready_replicas or 0
            desired = deployment.spec.replicas
            results[f"deployment_{name}"] = ready == desired
        
        # æ£€æŸ¥æœåŠ¡ç«¯ç‚¹
        for service in self.config['services']:
            try:
                response = requests.get(
                    f"http://{service['host']}/health",
                    timeout=5
                )
                results[f"service_{service['name']}"] = response.status_code == 200
            except:
                results[f"service_{service['name']}"] = False
        
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        results['database'] = self._check_database()
        
        # æ£€æŸ¥Redis
        results['redis'] = self._check_redis()
        
        return results
    
    def auto_scale(self) -> None:
        """åŸºäºè´Ÿè½½è‡ªåŠ¨æ‰©ç¼©å®¹"""
        metrics = self._get_metrics()
        
        for deployment_name in self.config['auto_scale']['deployments']:
            current_replicas = self._get_current_replicas(deployment_name)
            
            # è®¡ç®—ç›®æ ‡å‰¯æœ¬æ•°
            target_replicas = self._calculate_target_replicas(
                deployment_name,
                metrics,
                current_replicas
            )
            
            if target_replicas != current_replicas:
                logger.info(
                    f"Scaling {deployment_name} from {current_replicas} to {target_replicas}"
                )
                self._scale_deployment(deployment_name, target_replicas)
    
    def auto_restart_unhealthy(self) -> None:
        """è‡ªåŠ¨é‡å¯ä¸å¥åº·çš„Pod"""
        pods = self.k8s_core.list_namespaced_pod(
            namespace=self.config['namespace']
        )
        
        for pod in pods.items:
            if self._is_pod_unhealthy(pod):
                logger.warning(f"Restarting unhealthy pod: {pod.metadata.name}")
                self.k8s_core.delete_namespaced_pod(
                    name=pod.metadata.name,
                    namespace=self.config['namespace']
                )
    
    def cleanup_resources(self) -> None:
        """æ¸…ç†æ— ç”¨èµ„æº"""
        # æ¸…ç†å·²å®Œæˆçš„Job
        jobs = client.BatchV1Api().list_namespaced_job(
            namespace=self.config['namespace']
        )
        
        for job in jobs.items:
            if job.status.succeeded:
                age = datetime.now() - job.metadata.creation_timestamp.replace(tzinfo=None)
                if age > timedelta(days=7):
                    logger.info(f"Deleting old job: {job.metadata.name}")
                    client.BatchV1Api().delete_namespaced_job(
                        name=job.metadata.name,
                        namespace=self.config['namespace']
                    )
        
        # æ¸…ç†æ—§çš„ReplicaSet
        replicasets = self.k8s_apps.list_namespaced_replica_set(
            namespace=self.config['namespace']
        )
        
        for rs in replicasets.items:
            if rs.spec.replicas == 0 and rs.status.replicas == 0:
                age = datetime.now() - rs.metadata.creation_timestamp.replace(tzinfo=None)
                if age > timedelta(days=30):
                    logger.info(f"Deleting old replicaset: {rs.metadata.name}")
                    self.k8s_apps.delete_namespaced_replica_set(
                        name=rs.metadata.name,
                        namespace=self.config['namespace']
                    )
    
    def backup_configs(self) -> None:
        """å¤‡ä»½é…ç½®"""
        backup_dir = f"/backup/{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(backup_dir, exist_ok=True)
        
        # å¤‡ä»½ConfigMap
        configmaps = self.k8s_core.list_namespaced_config_map(
            namespace=self.config['namespace']
        )
        
        for cm in configmaps.items:
            with open(f"{backup_dir}/configmap_{cm.metadata.name}.yaml", 'w') as f:
                yaml.dump(cm.to_dict(), f)
        
        # å¤‡ä»½Secret
        secrets = self.k8s_core.list_namespaced_secret(
            namespace=self.config['namespace']
        )
        
        for secret in secrets.items:
            if secret.type != 'kubernetes.io/service-account-token':
                with open(f"{backup_dir}/secret_{secret.metadata.name}.yaml", 'w') as f:
                    # ä¸åŒ…å«å®é™…æ•°æ®ï¼Œåªä¿å­˜ç»“æ„
                    secret_dict = secret.to_dict()
                    secret_dict['data'] = {k: '***' for k in secret_dict.get('data', {})}
                    yaml.dump(secret_dict, f)
        
        logger.info(f"Configurations backed up to {backup_dir}")
    
    def log_analysis(self) -> Dict[str, int]:
        """æ—¥å¿—åˆ†æ"""
        error_patterns = self.config['log_analysis']['error_patterns']
        results = {pattern: 0 for pattern in error_patterns}
        
        pods = self.k8s_core.list_namespaced_pod(
            namespace=self.config['namespace']
        )
        
        for pod in pods.items:
            logs = self.k8s_core.read_namespaced_pod_log(
                name=pod.metadata.name,
                namespace=self.config['namespace'],
                tail_lines=1000
            )
            
            for pattern in error_patterns:
                results[pattern] += logs.count(pattern)
        
        return results
    
    def performance_report(self) -> Dict:
        """æ€§èƒ½æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'metrics': {}
        }
        
        # è·å–PrometheusæŒ‡æ ‡
        metrics_url = f"http://{self.config['prometheus']['host']}/api/v1/query"
        
        queries = {
            'cpu_usage': 'avg(rate(container_cpu_usage_seconds_total[5m]))',
            'memory_usage': 'avg(container_memory_usage_bytes)',
            'request_rate': 'sum(rate(http_requests_total[5m]))',
            'error_rate': 'sum(rate(http_requests_total{status=~"5.."}[5m]))',
            'p95_latency': 'histogram_quantile(0.95, http_request_duration_seconds_bucket)'
        }
        
        for name, query in queries.items():
            response = requests.get(metrics_url, params={'query': query})
            if response.status_code == 200:
                data = response.json()
                if data['data']['result']:
                    report['metrics'][name] = float(
                        data['data']['result'][0]['value'][1]
                    )
        
        return report
    
    def disaster_recovery_test(self) -> bool:
        """ç¾éš¾æ¢å¤æµ‹è¯•"""
        logger.info("Starting disaster recovery test...")
        
        # 1. å¤‡ä»½å½“å‰çŠ¶æ€
        self.backup_configs()
        
        # 2. æ¨¡æ‹Ÿæ•…éšœ
        test_deployment = self.config['dr_test']['test_deployment']
        original_replicas = self._get_current_replicas(test_deployment)
        
        # ç¼©å®¹åˆ°0
        self._scale_deployment(test_deployment, 0)
        time.sleep(10)
        
        # 3. æ¢å¤
        self._scale_deployment(test_deployment, original_replicas)
        time.sleep(30)
        
        # 4. éªŒè¯æ¢å¤
        health = self.health_check()
        
        if all(health.values()):
            logger.info("Disaster recovery test passed")
            return True
        else:
            logger.error("Disaster recovery test failed")
            return False
    
    def _check_database(self) -> bool:
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
        try:
            import pymysql
            conn = pymysql.connect(
                host=self.config['database']['host'],
                user=self.config['database']['user'],
                password=os.getenv('DB_PASSWORD'),
                database=self.config['database']['name']
            )
            with conn.cursor() as cursor:
                cursor.execute("SELECT 1")
            conn.close()
            return True
        except:
            return False
    
    def _check_redis(self) -> bool:
        """æ£€æŸ¥Redisè¿æ¥"""
        try:
            import redis
            r = redis.Redis(
                host=self.config['redis']['host'],
                port=self.config['redis']['port'],
                password=os.getenv('REDIS_PASSWORD')
            )
            r.ping()
            return True
        except:
            return False
    
    def _get_metrics(self) -> Dict:
        """è·å–ç³»ç»ŸæŒ‡æ ‡"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä»Prometheusè·å–
        return {
            'cpu_usage': 0.7,
            'memory_usage': 0.6,
            'request_rate': 100
        }
    
    def _get_current_replicas(self, deployment_name: str) -> int:
        """è·å–å½“å‰å‰¯æœ¬æ•°"""
        deployment = self.k8s_apps.read_namespaced_deployment(
            name=deployment_name,
            namespace=self.config['namespace']
        )
        return deployment.spec.replicas
    
    def _calculate_target_replicas(
        self,
        deployment_name: str,
        metrics: Dict,
        current: int
    ) -> int:
        """è®¡ç®—ç›®æ ‡å‰¯æœ¬æ•°"""
        config = self.config['auto_scale']
        min_replicas = config['min_replicas']
        max_replicas = config['max_replicas']
        
        # åŸºäºCPUä½¿ç”¨ç‡
        if metrics['cpu_usage'] > 0.8:
            target = min(current + 2, max_replicas)
        elif metrics['cpu_usage'] < 0.3:
            target = max(current - 1, min_replicas)
        else:
            target = current
        
        return target
    
    def _scale_deployment(self, name: str, replicas: int) -> None:
        """æ‰©ç¼©å®¹éƒ¨ç½²"""
        body = {'spec': {'replicas': replicas}}
        self.k8s_apps.patch_namespaced_deployment_scale(
            name=name,
            namespace=self.config['namespace'],
            body=body
        )
    
    def _is_pod_unhealthy(self, pod) -> bool:
        """åˆ¤æ–­Podæ˜¯å¦ä¸å¥åº·"""
        if pod.status.phase != 'Running':
            return False
        
        # æ£€æŸ¥é‡å¯æ¬¡æ•°
        for container in pod.status.container_statuses or []:
            if container.restart_count > 5:
                return True
        
        return False


def main():
    ops = OpsAutomation('config/ops.yaml')
    
    # å®šæœŸæ‰§è¡Œçš„ä»»åŠ¡
    while True:
        try:
            # å¥åº·æ£€æŸ¥
            health = ops.health_check()
            logger.info(f"Health check: {health}")
            
            # è‡ªåŠ¨æ‰©ç¼©å®¹
            ops.auto_scale()
            
            # æ¸…ç†ä¸å¥åº·çš„Pod
            ops.auto_restart_unhealthy()
            
            # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡çš„ä»»åŠ¡
            if datetime.now().minute == 0:
                ops.cleanup_resources()
                ops.performance_report()
            
            # æ¯å¤©æ‰§è¡Œä¸€æ¬¡çš„ä»»åŠ¡
            if datetime.now().hour == 2 and datetime.now().minute == 0:
                ops.backup_configs()
                ops.log_analysis()
            
            # æ¯å‘¨æ‰§è¡Œä¸€æ¬¡ç¾éš¾æ¢å¤æµ‹è¯•
            if datetime.now().weekday() == 0 and datetime.now().hour == 3:
                ops.disaster_recovery_test()
            
        except Exception as e:
            logger.error(f"Error in ops automation: {e}")
        
        time.sleep(60)  # æ¯åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡


if __name__ == "__main__":
    main()
```

## ğŸ“Š Phase 8 å®Œæˆæ ‡å‡†æ£€æŸ¥æ¸…å•

### æŠ€æœ¯æŒ‡æ ‡
- [ ] CI/CDæµæ°´çº¿è¦†ç›–æ‰€æœ‰ç¯å¢ƒ
- [ ] è‡ªåŠ¨åŒ–æµ‹è¯•é€šè¿‡ç‡ > 95%
- [ ] éƒ¨ç½²æˆåŠŸç‡ > 99%
- [ ] å›æ»šæ—¶é—´ < 5åˆ†é’Ÿ
- [ ] å®¹å™¨å¯åŠ¨æ—¶é—´ < 30ç§’

### åŠŸèƒ½éªŒè¯
- [ ] ä»£ç æäº¤è‡ªåŠ¨è§¦å‘æ„å»º
- [ ] æµ‹è¯•å¤±è´¥è‡ªåŠ¨é˜»æ­¢éƒ¨ç½²
- [ ] è“ç»¿éƒ¨ç½²æ­£å¸¸åˆ‡æ¢
- [ ] é‡‘ä¸é›€å‘å¸ƒæµé‡æ§åˆ¶å‡†ç¡®
- [ ] è‡ªåŠ¨æ‰©ç¼©å®¹å“åº”åŠæ—¶

### è¿ç»´æŒ‡æ ‡
- [ ] éƒ¨ç½²æ–‡æ¡£å®Œæ•´
- [ ] å›æ»šæµç¨‹æ¸…æ™°
- [ ] ç›‘æ§å‘Šè­¦é…ç½®å®Œå–„
- [ ] æ—¥å¿—æ”¶é›†æ­£å¸¸

## ğŸš€ Phase 8 äº¤ä»˜ç‰©

1. **CI/CDé…ç½®**
   - GitLab CIå®Œæ•´é…ç½®
   - Jenkins Pipelineï¼ˆå¤‡é€‰ï¼‰
   - GitHub Actionsï¼ˆå¤‡é€‰ï¼‰

2. **å®¹å™¨åŒ–**
   - ä¼˜åŒ–çš„Dockerfile
   - docker-compose.yml
   - é•œåƒå®‰å…¨æ‰«æé…ç½®

3. **Kubernetesé…ç½®**
   - Deploymenté…ç½®
   - Service/Ingressé…ç½®
   - HPAè‡ªåŠ¨æ‰©ç¼©å®¹
   - ConfigMap/Secretç®¡ç†

4. **éƒ¨ç½²è„šæœ¬**
   - è“ç»¿éƒ¨ç½²è„šæœ¬
   - é‡‘ä¸é›€å‘å¸ƒè„šæœ¬
   - å›æ»šè„šæœ¬
   - å¥åº·æ£€æŸ¥è„šæœ¬

5. **è¿ç»´å·¥å…·**
   - è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬
   - æ—¥å¿—åˆ†æå·¥å…·
   - æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ
   - ç¾éš¾æ¢å¤æµ‹è¯•

6. **æ–‡æ¡£**
   - CI/CD-GUIDE.md
   - DEPLOYMENT-MANUAL.md
   - KUBERNETES-OPERATIONS.md
   - DISASTER-RECOVERY.md

è¿™å°±æ˜¯Phase 8çš„å®Œæ•´å®æ–½æ–¹æ¡ˆï¼Œæ¶µç›–äº†ä»ä»£ç æäº¤åˆ°ç”Ÿäº§éƒ¨ç½²çš„å®Œæ•´è‡ªåŠ¨åŒ–æµç¨‹ï¼ŒåŒ…æ‹¬CI/CDã€å®¹å™¨åŒ–ã€Kubernetesç¼–æ’å’Œè‡ªåŠ¨åŒ–è¿ç»´ç­‰å…³é”®ç¯èŠ‚ã€‚